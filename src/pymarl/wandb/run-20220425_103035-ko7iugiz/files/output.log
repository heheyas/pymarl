[INFO 10:30:43] my_main Experiment Parameters:
[INFO 10:30:43] my_main
{   'action_selector': 'epsilon_greedy',
    'agent': 'rnn',
    'agent_output_type': 'q',
    'batch_size': 128,
    'batch_size_run': 30,
    'buffer_cpu_only': True,
    'buffer_size': 5000,
    'checkpoint_path': '',
    'critic_lr': 0.001,
    'double_q': True,
    'env': 'overcooked',
    'env_args': {   'env_config': {   'env_params': {   'horizon': 400},
                                      'mdp_params': {   'layout_name': 'cramped_room',
                                                        'rew_shaping_params': {   'DISH_DISP_DISTANCE_REW': 0,
                                                                                  'DISH_PICKUP_REWARD': 3,
                                                                                  'PLACEMENT_IN_POT_REW': 3,
                                                                                  'POT_DISTANCE_REW': 0,
                                                                                  'SOUP_DISTANCE_REW': 0,
                                                                                  'SOUP_PICKUP_REWARD': 5}},
                                      'multi_agent_params': {   'bc_schedule': [   [   0,
                                                                                       0],
                                                                                   [   inf,
                                                                                       0]],
                                                                'reward_shaping_factor': 1.0,
                                                                'reward_shaping_horizon': 2500000,
                                                                'use_phi': False}},
                    'seed': 293919782},
    'epsilon_anneal_time': 5000000,
    'epsilon_finish': 0.05,
    'epsilon_start': 1.0,
    'evaluate': False,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'hypernet_embed': 64,
    'hypernet_layers': 2,
    'label': 'default_label',
    'learner': 'q_learner',
    'learner_log_interval': 10000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 10000,
    'lr': 0.001,
    'mac': 'basic_mac',
    'mixer': 'qmix',
    'mixing_embed_dim': 32,
    'name': 'qmix_parallel',
    'obs_agent_id': True,
    'obs_last_action': True,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'repeat_id': 1,
    'rnn_hidden_dim': 32,
    'runner': 'parallel',
    'runner_log_interval': 10000,
    'save_model': False,
    'save_model_interval': 2000000,
    'save_replay': False,
    'seed': 293919782,
    't_max': 10000000,
    'target_update_interval': 200,
    'test_greedy': True,
    'test_interval': 10000,
    'test_nepisode': 30,
    'use_cuda': True,
    'use_tensorboard': True}
[INFO 10:30:48] my_main Beginning training for 10000000 timesteps
/opt/czl/libs/pymarl/src/components/episode_buffer.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)
  v = th.tensor(v, dtype=dtype, device=self.device)
/opt/czl/libs/pymarl/src/components/episode_buffer.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
